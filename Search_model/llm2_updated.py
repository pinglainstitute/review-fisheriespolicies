# llm2_updated.py
import os
import streamlit as st

# --- deps æ›´æ–°ï¼šç”¨ pypdf æ›¿ä»£ PyPDF2ï¼ŒFAISS ä»ç¤¾åŒºåŒ…å¯¼å…¥ ---
from pypdf import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter

from dotenv import load_dotenv
load_dotenv()

# Google Generative AI / LangChain é›†æˆ
from langchain_google_genai import (
    GoogleGenerativeAIEmbeddings,
    ChatGoogleGenerativeAI,
)
import google.generativeai as genai

# FAISS æ”¹ä¸ºç¤¾åŒºåŒ…è·¯å¾„ï¼ˆ0.1+ååˆ†åŒ…ï¼‰
from langchain_community.vectorstores import FAISS

from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate

from google import genai
from google.genai import types

# ---------- åˆå§‹åŒ– ----------
client = genai.Client()

API_KEY = os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    st.error("API key is missing. Please set GOOGLE_API_KEY in your .env or environment.")
else:
    genai.configure(api_key=API_KEY)

INDEX_DIR = "faiss_index"  # æŒä¹…åŒ–è·¯å¾„
EMBED_MODEL = "text-embedding-004"  # å»ºè®®ä½¿ç”¨æœ€æ–°ç‰ˆï¼›æ—§å€¼ models/embedding-001 å·²è¿‡æ—¶


# ---------- å·¥å…·å‡½æ•° ----------
def get_pdf_text(pdf_docs):
    """å°†ä¸Šä¼ çš„å¤šä¸ª PDF åˆå¹¶ä¸ºçº¯æ–‡æœ¬ã€‚"""
    text = []
    for pdf in pdf_docs:
        reader = PdfReader(pdf)
        for page in reader.pages:
            page_text = page.extract_text() or ""
            if page_text.strip():
                text.append(page_text)
    return "\n".join(text)


def get_text_chunks(text):
    """åˆ†å—ç­–ç•¥ï¼šè¾ƒå°å—+éƒ¨åˆ†é‡å ï¼Œæå‡å¬å›ä¸ä¸Šä¸‹æ–‡è¿è´¯ã€‚"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=200,
        length_function=len,
    )
    return splitter.split_text(text)


@st.cache_resource(show_spinner=False)
def build_or_load_vector_store(_chunks=None):
    """
    - å¦‚æœå·²æœ‰æœ¬åœ°ç´¢å¼•ï¼šåŠ è½½
    - å¦åˆ™ï¼šç”¨ä¼ å…¥çš„ chunks æ„å»ºå¹¶ä¿å­˜
    """
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBED_MODEL)

    # å…ˆå°è¯•åŠ è½½
    if os.path.isdir(INDEX_DIR):
        try:
            # ä¸ä½¿ç”¨ allow_dangerous_deserializationï¼Œé¿å…è€ç‰ˆæœ¬ pickle é£é™©
            return FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=False)
        except Exception:
            # ç´¢å¼•æ ¼å¼ä¸å…¼å®¹æ—¶ï¼Œé‡å»º
            pass

    # æ„å»ºæ–°ç´¢å¼•
    if not _chunks:
        raise ValueError("No chunks provided to build a new FAISS index.")
    vs = FAISS.from_texts(_chunks, embedding=embeddings)
    vs.save_local(INDEX_DIR)
    return vs


def get_conversational_chain():
    """æœ€å°æ”¹åŠ¨æ²¿ç”¨ load_qa_chainï¼ˆstuffï¼‰ï¼Œåç»­å¯é€æ­¥è¿ç§»åˆ° LCELã€‚"""
    prompt_template = """
Understand the question and answer strictly based on the provided context.
Write a step-by-step, precise, and well-structured answer.

Context:
{context}

Question:
{question}

Answer:
    """.strip()

    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro-002",
        temperature=0.2,  # æ›´ç¨³å®šçš„å›ç­”ï¼›ä¹‹å‰ 1.2 åå‘æ•£
    )
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = load_qa_chain(llm, chain_type="stuff", prompt=prompt)
    return chain


# def answer_question(vs, user_question: str):
#     """ç›¸ä¼¼åº¦æ£€ç´¢ + QA é“¾"""
#     docs = vs.similarity_search(user_question, k=4)
#     chain = get_conversational_chain()
#     result = chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)
#     return result.get("output_text", "").strip() or "No output produced."
def answer_question(vs, user_question: str):
    """ç›¸ä¼¼åº¦æ£€ç´¢ + Gemini 2.5 Flash (Thinking)"""
    docs = vs.similarity_search(user_question, k=4)
    context_text = "\n\n".join([d.page_content for d in docs])

    # è‡ªå®šä¹‰ prompt
    prompt = f"""
    You are an expert fisheries policy assistant. 
    Answer the question using ONLY the following context. 
    Be precise and structured. If not found, say "I don't know."

    Context:
    {context_text}

    Question:
    {user_question}
    """

    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
        config=types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(thinking_budget=50)  # ğŸ’¡ å¼€å¯æ€è€ƒ
        ),
    )
    return response.text.strip() if response.text else "No output produced."


# ---------- Streamlit UI ----------
def main():
    st.set_page_config(page_title="AI Fisheries Manager ğŸŸ", page_icon="ğŸŸ")
    st.title("AI Fisheries Manager ğŸŸ")
    st.image("https://pingla.org.au/images/Pingala_Logo_option_7.png", use_container_width=False)

    # ä¾§è¾¹æ ï¼šä¸Šä¼ ä¸å»ºç´¢å¼•
    with st.sidebar:
        st.header("Documents")
        pdf_docs = st.file_uploader(
            "Upload PDF files then click 'Submit & Process'",
            type=["pdf"],
            accept_multiple_files=True,
        )
        if st.button("Submit & Process", type="primary", help="Extract, chunk, and index PDFs"):
            if not pdf_docs:
                st.error("Please upload at least one PDF.")
            else:
                with st.spinner("Extracting & indexing..."):
                    raw_text = get_pdf_text(pdf_docs)
                    if not raw_text.strip():
                        st.error("No extractable text found in the PDFs.")
                    else:
                        chunks = get_text_chunks(raw_text)
                        # å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼šå…ˆæ¸…æ‰æ—§ç›®å½•ï¼ˆé¿å…ä¸åŒç‰ˆæœ¬æ®‹ç•™ï¼‰
                        if os.path.isdir(INDEX_DIR):
                            import shutil
                            shutil.rmtree(INDEX_DIR, ignore_errors=True)
                        _ = build_or_load_vector_store(chunks)  # æ„å»ºå¹¶ç¼“å­˜
                        st.success("Index built successfully âœ…")

        st.divider()
        if os.path.isdir(INDEX_DIR):
            st.caption("FAISS index detected âœ”ï¸")
        else:
            st.caption("No index yet. Please upload PDFs and build the index.")

    # ä¸»åŒºï¼šé—®ç­”
    user_q = st.text_input("Ask the fisheries manager a question")
    if user_q:
        if not API_KEY:
            st.error("Missing GOOGLE_API_KEY.")
            return
        if not os.path.isdir(INDEX_DIR):
            st.warning("No index found. Please upload PDFs and click 'Submit & Process' first.")
            return
        with st.spinner("Retrieving & answering..."):
            vs = build_or_load_vector_store()
            answer = answer_question(vs, user_q)
        st.markdown("**Reply:**")
        st.write(answer)


if __name__ == "__main__":
    main()

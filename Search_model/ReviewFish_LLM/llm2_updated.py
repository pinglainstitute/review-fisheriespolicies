# llm2_updated.py
import os
import shutil
import streamlit as st

from dotenv import load_dotenv
load_dotenv()

# ---------- Keys & Client ----------
# # ç»Ÿä¸€è¯»å– KEYï¼›LangChain çš„ Google embeddings ä¹ æƒ¯è¯» GOOGLE_API_KEY
# API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
# if not API_KEY:
#     st.error("API key is missing. Set GEMINI_API_KEY or GOOGLE_API_KEY in .env / env.")
#     st.stop()

# # ç¡®ä¿ LangChain èƒ½è¯»åˆ°ï¼ˆéœ€è¦ GOOGLE_API_KEY è¿™ä¸ªå˜é‡åï¼‰
# os.environ["GOOGLE_API_KEY"] = API_KEY

# # æ–°ç‰ˆ Google GenAI å®¢æˆ·ç«¯ï¼ˆä¸å†ä½¿ç”¨ genai.configureï¼‰
# from google import genai
# from google.genai import types
#client = genai.Client(api_key=API_KEY)
# this version is modefied on 19 Novs
# ---------- Keys & Client ----------
API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
if not API_KEY:
    st.error("API key is missing. Set GEMINI_API_KEY or GOOGLE_API_KEY in .env / env.")
    st.stop()

os.environ["GOOGLE_API_KEY"] = API_KEY

import google.generativeai as genai
from google.generativeai import types

genai.configure(api_key=API_KEY)




# ---------- LangChain / PDF / Vector store ----------
from pypdf import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings

#ï¼ˆå¦‚æœä½ æƒ³åˆ‡æ¢åˆ° LangChain çš„å¯¹è¯é“¾ï¼Œä¿ç•™è¿™ä¸¤ä¸ªï¼›å½“å‰ç¤ºä¾‹ç›´æ¥èµ° client è°ƒç”¨ï¼‰
# from langchain.chains.question_answering import load_qa_chain
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain.prompts import PromptTemplate

INDEX_DIR = "faiss_index"
EMBED_MODEL = "models/text-embedding-004"   # æ³¨æ„å¸¦ models/ å‰ç¼€

# ---------- Helpers ----------
# def get_docs_with_meta(pdf_files):
#     """è¯»å–å¤šä¸ª PDFï¼Œé€é¡µæå–ä¸º Documentï¼Œå¹¶ä¿ç•™æ¥æºä¸é¡µç ã€‚"""
#     docs = []
#     for f in pdf_files:
#         reader = PdfReader(f)
#         name = getattr(f, "name", "uploaded.pdf")
#         for i, page in enumerate(reader.pages):
#             txt = (page.extract_text() or "").strip()
#             if not txt:
#                 continue
#             docs.append(
#                 Document(
#                     page_content=txt,
#                     metadata={"source": name, "page": i + 1}
#                 )
#             )
#     return docs
# ---------- Helpers ---------- optimize
def get_docs_with_meta(pdf_files):
    """è¯»å–å¤šä¸ª PDFï¼Œé€é¡µæå–ä¸º Documentï¼Œå¹¶ä¿ç•™æ¥æºä¸é¡µç ï¼›å¢åŠ æ–‡æœ¬æ¸…æ´—ã€‚"""
    docs = []
    for f in pdf_files:
        reader = PdfReader(f)
        name = getattr(f, "name", "uploaded.pdf")

        for i, page in enumerate(reader.pages):
            # æå– + æ¸…æ´—æ–‡æœ¬
            raw_txt = page.extract_text() or ""
            # å»æ‰å¤šä½™ç©ºç™½ä¸æ¢è¡Œï¼Œç»Ÿä¸€æˆä¸€è¡Œ
            txt = " ".join(raw_txt.split())

            # æçŸ­å†…å®¹ä¸€èˆ¬æ˜¯é¡µçœ‰é¡µè„š/ç©ºé¡µï¼Œç›´æ¥è·³è¿‡
            if len(txt) < 40:
                continue

            docs.append(
                Document(
                    page_content=txt,
                    metadata={"source": name, "page": i + 1}
                )
            )
    return docs

@st.cache_resource(show_spinner=False)
def build_or_load_vector_store(_documents=None):
    """
    è‹¥æœ¬åœ°å·²æœ‰ç´¢å¼•åˆ™åŠ è½½ï¼›å¦åˆ™ç”¨ä¼ å…¥çš„ documents æ„å»ºå¹¶ä¿å­˜ã€‚
    """
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBED_MODEL)

    if os.path.isdir(INDEX_DIR):
        try:
            return FAISS.load_local(
                INDEX_DIR, embeddings, allow_dangerous_deserialization=False
            )
        except Exception:
            pass  # ç‰ˆæœ¬æˆ–æ ¼å¼ä¸å…¼å®¹ï¼Œèµ°é‡å»º

    if not _documents:
        raise ValueError("No documents provided to build a new FAISS index.")

    vs = FAISS.from_documents(_documents, embedding=embeddings)
    vs.save_local(INDEX_DIR)
    return vs


# def answer_question(vs, user_question: str):
#     """ç›¸ä¼¼åº¦æ£€ç´¢ + Gemini 2.5 Flashï¼ˆå¯é€‰ Thinkingï¼‰"""
#     docs = vs.similarity_search(user_question, k=8) #improve
#     context_text = "\n\n".join([d.page_content for d in docs])

#     prompt = f"""
# You are an expert fisheries policy assistant.
# Answer the question using ONLY the following context.
# Be precise and structured. If not found, say "I don't know."

# Context:
# {context_text}

# Question:
# {user_question}
# """.strip()

#     resp = client.models.generate_content(
#         model="gemini-2.5-flash",
#         contents=prompt,
#         config=types.GenerateContentConfig(
#             thinking_config=types.ThinkingConfig(thinking_budget=30)  # ä½ä¸€äº›ä»¥æé€Ÿ/çœè´¹
#         ),
#     )
#     answer = (resp.text or "").strip() if hasattr(resp, "text") else ""
#     if not answer:
#         answer = "No output produced."

#     # è¿”å›ç­”æ¡ˆä¸æ¥æºï¼Œæ–¹ä¾¿ UI å±•ç¤º
#     sources = [(d.metadata.get("source"), d.metadata.get("page")) for d in docs]
#     return answer, sources
def robust_retrieve(vs, query, k=10):
    """æ›´ç¨³å¥çš„ MMR æ£€ç´¢é€»è¾‘ï¼Œæ¸…æ™°åˆ†å±‚ã€‚ similarity_with_score, fallback"""
    try:
        # å…ˆæ‹¿åˆæ­¥å€™é€‰ ï¼ˆæŒ‰ç›¸ä¼¼åº¦å¾—åˆ†æ’åºï¼‰
        pairs = vs.similarity_search_with_score(query, k=30)
        pairs = sorted(pairs, key=lambda x: x[1])
        top_docs = [d for d, _ in pairs[:12]]

        # å†è·‘ä¸€è½® MMR å»é‡ï¼Œç”¨MMRå»å†—ä½™
        mmr_docs = vs.max_marginal_relevance_search(
            query, k=k, fetch_k=30, lambda_mult=0.2
        )
        return mmr_docs or top_docs

    except Exception as e:
        # fallback: ç®€å• similarity_search
        st.warning(f"MMR retrieval failed, falling back to similarity_search. ({e})")
        return vs.similarity_search(query, k=k)
    
def answer_question(vs, user_question: str):

    """æ›´ç¨³å¥çš„æ£€ç´¢ + ç»“æ„åŒ–å›ç­”LLMç”Ÿæˆ + å¼•ç”¨æ ‡æ³¨"""

    docs = robust_retrieve(vs, user_question, k=10)
    if not docs:
        # return "I don't know.", []
        st.warning("âš ï¸ No documents retrieved for this query.")

    # # 1) ç”¨ MMRï¼Œå…ˆæŠ“æ›´å¤šå€™é€‰ï¼ˆfetch_kï¼‰ï¼Œå†å»å†—ä½™
    # try:
    #     docs = vs.max_marginal_relevance_search(
    #         user_question, k=10, fetch_k=40, lambda_mult=0.2
    #     )
    # except Exception:
    #     # å…¼å®¹æ²¡æœ‰ MMR çš„ç´¢å¼•
    #     docs = vs.similarity_search(user_question, k=12)

    #---------------------------------version1-----------------------------------#
    '''
    try:
        pairs = vs.similarity_search_with_score(user_question, k=30)
        # LangChain-FAISS çš„å¾—åˆ†å«ä¹‰éšç´¢å¼•ç±»å‹ä¸åŒï¼Œè¿™é‡Œä»…åšæ’åºä¸å†ç­›
        pairs = sorted(pairs, key=lambda x: x[1])  # åˆ†æ•°å°=æ›´è¿‘ï¼ˆå¸¸è§æƒ…å½¢ï¼‰
        docs = [d for d, _ in pairs[:12]]
        # å†è·‘ä¸€è½® MMR å»å†—ä½™
        docs = vs.max_marginal_relevance_search(user_question, k=10, fetch_k=30, lambda_mult=0.2)
    except Exception:
        docs = vs.max_marginal_relevance_search(user_question, k=10, fetch_k=40, lambda_mult=0.2)
    '''

    #---------------------------------version2-----------------------------------#
    


    # 3) æŠŠæ¥æºé¡µç åµŒè¿›ä¸Šä¸‹æ–‡ï¼Œé¼“åŠ±æ¨¡å‹å¼•ç”¨
    blocks, used = [], []
    MAX_CTX = 12000  # é˜²æ­¢è¿‡é•¿è¢«æˆªæ–­
    total = 0
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source", "PDF")
        pg  = d.metadata.get("page", "?")
        block = f"[S{i} | {src} p.{pg}]\n{d.page_content.strip()}"
        if total + len(block) > MAX_CTX:
            break
        blocks.append(block)
        used.append((src, pg))
        total += len(block)

    context_text = "\n\n".join(blocks)

    # 4) æ›´â€œè¿›å–â€çš„æç¤ºï¼šæœ‰è¯æ®å°±ä½œç­”ï¼Œå¹¶åœ¨å¥å°¾æ‰“ [S#] å¼•ç”¨
    prompt = f"""
You are an expert fisheries policy assistant.
Use ONLY the context below. If the context contains relevant facts, answer concisely and cite sources like [S1], [S2] at the end of sentences derived from them.
Only say "I don't know." if the context truly does not contain the answer.

Context:
{context_text}

Question:
{user_question}

Answer (with citations):
""".strip()

    # resp = client.models.generate_content(
    #     model="gemini-2.5-flash",
    #     contents=prompt,
    #     config=types.GenerateContentConfig(
    #         thinking_config=types.ThinkingConfig(thinking_budget=20)
    #     ),
    # âœ… åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = genai.GenerativeModel("gemini-2.5-flash")

    # âœ… è°ƒç”¨ generate_content
    resp = model.generate_content(
        prompt,
        generation_config={
            "temperature": 0.2,
            "max_output_tokens": 1024,
        },
    )
    # æå–å›ç­”æ–‡æœ¬
    answer = (getattr(resp, "text", "") or "").strip()
    if not answer:
        answer = "I don't know."
 
    
    # âœ… åœ¨è¿™é‡ŒåŠ â€œæœ€è¿‘ç‰‡æ®µæç¤ºâ€é€»è¾‘
    if answer.strip() == "I don't know." and used:
        hints = [f"â€¢ {s or 'PDF'} p.{p}" for s,p in used[:3]]
        st.info("Closest sections:\n" + "\n".join(hints))

    return answer, used

@st.cache_resource(show_spinner=False)
def build_or_load_vector_store(_documents=None):
    """
    ä¼˜å…ˆåŠ è½½å·²æœ‰ç´¢å¼•ï¼›å¦åˆ™åœ¨æä¾›äº† documents æ—¶æ„å»ºã€‚
    è‹¥æ—¢æ²¡æœ‰ç´¢å¼•ã€ä¹Ÿæ²¡ä¼ å…¥ documentsï¼Œåˆ™è¿”å› Noneï¼ˆä¸æŠ›å¼‚å¸¸ï¼‰ã€‚
    """
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBED_MODEL)

    # å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
    if os.path.isdir(INDEX_DIR):
        try:
            return FAISS.load_local(
                INDEX_DIR, embeddings, allow_dangerous_deserialization=False
            )
        except Exception:
            # ç‰ˆæœ¬æˆ–æ ¼å¼ä¸å…¼å®¹ï¼Œç»§ç»­å°è¯•é‡å»º
            pass

    # æ²¡æœ‰æœ¬åœ°ç´¢å¼•ï¼›è‹¥æä¾›äº†æ–‡æ¡£åˆ™æ„å»ºï¼Œå¦åˆ™è¿”å› None
    if _documents:
        vs = FAISS.from_documents(_documents, embedding=embeddings)
        vs.save_local(INDEX_DIR)
        return vs

    # æ—¢æ— ç´¢å¼•ï¼Œä¹Ÿæ²¡æ–‡æ¡£ â€”â€” äº¤ç»™ä¸Šå±‚å‹å¥½æç¤º
    return None

# ---------- Streamlit UI ----------
def main():
    st.set_page_config(page_title="AI Fisheries Manager ğŸŸ", page_icon="ğŸŸ")
    st.title("AI Fisheries Manager ğŸŸ")
    st.image("https://pingla.org.au/images/Pingala_Logo_option_7.png", width=300)

    # ä¾§è¾¹æ ï¼šä¸Šä¼ ä¸å»ºç´¢å¼•
    with st.sidebar:
        st.header("Documents")
        pdf_docs = st.file_uploader(
            "Upload PDF files then click 'Submit & Process'",
            type=["pdf"],
            accept_multiple_files=True,
        )

        if st.button("Submit & Process", type="primary", help="Extract, chunk, and index PDFs"):
            if not pdf_docs:
                st.error("Please upload at least one PDF.")
            else:
                with st.spinner("Extracting & indexing..."):
                    raw_docs = get_docs_with_meta(pdf_docs)
                    if not raw_docs:
                        st.error("No extractable text found in the PDFs.")
                    else:
                        splitter = RecursiveCharacterTextSplitter(
                            # chunk_size=2000, chunk_overlap=200 ## optimize
                            chunk_size=2500,   # é€‚åº¦æ”¾å¤§
                            chunk_overlap=300, # ä¿ç•™è·¨æ®µè¯­ä¹‰
                            length_function=len,
                        )
                        chunks = splitter.split_documents(raw_docs)

                        # é‡å»ºç´¢å¼•ï¼šå…ˆæ¸…æ‰æ—§ç›®å½•
                        if os.path.isdir(INDEX_DIR):
                            shutil.rmtree(INDEX_DIR, ignore_errors=True)

                        # _ = build_or_load_vector_store(chunks)  #optimize
                        # st.success("Index built successfully âœ…")
                        # æäº¤åæ„å»º
                        vs = build_or_load_vector_store(chunks)
                        if vs is None:
                            st.error("Index build failed. Please re-upload PDFs and click 'Submit & Process' again.")
                        else:
                            st.success("Index built successfully âœ…")

        st.divider()
        st.caption("FAISS index detected âœ”ï¸" if os.path.isdir(INDEX_DIR) else
                   "No index yet. Please upload PDFs and build the index.")

    # ä¸»åŒºï¼šé—®ç­”
    user_q = st.text_input("Ask the fisheries manager a question")
    if user_q:
        if not os.path.isdir(INDEX_DIR):
            st.warning("No index found. Please upload PDFs and click 'Submit & Process' first.")
            return
        with st.spinner("Retrieving & answering..."):
            vs = build_or_load_vector_store()  ##optimize 
            if vs is None:
                st.warning("Index not ready. Please upload PDFs and click 'Submit & Process' first.")
                st.stop()
                
            answer, sources = answer_question(vs, user_q)

        st.markdown("**Reply:**")
        st.write(answer)

        if sources:
            src_text = ", ".join([f"{s or 'PDF'} p.{p}" for s, p in sources])
            st.caption(f"Sources: {src_text}")


if __name__ == "__main__":
    main()
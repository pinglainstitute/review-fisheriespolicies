# llm2_updated.py
import os
import shutil
import streamlit as st

from dotenv import load_dotenv
load_dotenv()

# ---------- é…ç½® API å¯†é’¥ ----------
API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
if not API_KEY:
    st.error("API key is missing. Set GEMINI_API_KEY or GOOGLE_API_KEY in .env / env.")
    st.stop()

os.environ["GOOGLE_API_KEY"] = API_KEY

import google.generativeai as genai
from google.generativeai import types

genai.configure(api_key=API_KEY)




# ---------- LangChain / PDF / Vector store ----------
from pypdf import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings

#ï¼ˆå¦‚æœä½ æƒ³åˆ‡æ¢åˆ° LangChain çš„å¯¹è¯é“¾ï¼Œä¿ç•™è¿™ä¸¤ä¸ªï¼›å½“å‰ç¤ºä¾‹ç›´æ¥èµ° client è°ƒç”¨ï¼‰
# from langchain.chains.question_answering import load_qa_chain
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain.prompts import PromptTemplate

INDEX_DIR = "faiss_index"
EMBED_MODEL = "models/text-embedding-004"   # æ³¨æ„å¸¦ models/ å‰ç¼€

# ---------- è¾…åŠ©å‡½æ•° ----------
def get_docs_with_meta(pdf_files):
    """Read multiple PDFs, extract page by page as Documents, preserve source and page numbers; with text cleaning."""
    docs = []
    for f in pdf_files:
        reader = PdfReader(f)
        name = getattr(f, "name", "uploaded.pdf")

        for i, page in enumerate(reader.pages):
            # æå– + æ¸…æ´—æ–‡æœ¬
            raw_txt = page.extract_text() or ""
            # å»æ‰å¤šä½™ç©ºç™½ä¸æ¢è¡Œï¼Œç»Ÿä¸€æˆä¸€è¡Œ
            txt = " ".join(raw_txt.split())

            # æçŸ­å†…å®¹ä¸€èˆ¬æ˜¯é¡µçœ‰é¡µè„š/ç©ºé¡µï¼Œç›´æ¥è·³è¿‡
            if len(txt) < 40:
                continue

            docs.append(
                Document(
                    page_content=txt,
                    metadata={"source": name, "page": i + 1}
                )
            )
    return docs

# def answer_question(vs, user_question: str):
#     """ç›¸ä¼¼åº¦æ£€ç´¢ + Gemini 2.5 Flashï¼ˆå¯é€‰ Thinkingï¼‰"""
#     docs = vs.similarity_search(user_question, k=8) #improve
#     context_text = "\n\n".join([d.page_content for d in docs])

#     prompt = f"""
# You are an expert fisheries policy assistant.
# Answer the question using ONLY the following context.
# Be precise and structured. If not found, say "I don't know."

# Context:
# {context_text}

# Question:
# {user_question}
# """.strip()

#     resp = client.models.generate_content(
#         model="gemini-2.5-flash",
#         contents=prompt,
#         config=types.GenerateContentConfig(
#             thinking_config=types.ThinkingConfig(thinking_budget=30)  # ä½ä¸€äº›ä»¥æé€Ÿ/çœè´¹
#         ),
#     )
#     answer = (resp.text or "").strip() if hasattr(resp, "text") else ""
#     if not answer:
#         answer = "No output produced."

#     # è¿”å›ç­”æ¡ˆä¸æ¥æºï¼Œæ–¹ä¾¿ UI å±•ç¤º
#     sources = [(d.metadata.get("source"), d.metadata.get("page")) for d in docs]
#     return answer, sources
def robust_retrieve(vs, query, k=10):
    """More robust MMR retrieval logic with clear layering. similarity_with_score, fallback"""
    try:
        # å…ˆæ‹¿åˆæ­¥å€™é€‰ ï¼ˆæŒ‰ç›¸ä¼¼åº¦å¾—åˆ†æ’åºï¼‰
        pairs = vs.similarity_search_with_score(query, k=30)
        pairs = sorted(pairs, key=lambda x: x[1])
        top_docs = [d for d, _ in pairs[:12]]

        # å†è·‘ä¸€è½® MMR å»é‡ï¼Œç”¨MMRå»å†—ä½™
        mmr_docs = vs.max_marginal_relevance_search(
            query, k=k, fetch_k=30, lambda_mult=0.2
        )
        return mmr_docs or top_docs

    except Exception as e:
        # fallback: ç®€å• similarity_search
        st.warning(f"MMR retrieval failed, falling back to similarity_search. ({e})")
        return vs.similarity_search(query, k=k)
    
def answer_question(vs, user_question: str):

    """More robust retrieval + structured LLM answer generation + citation markup"""

    docs = robust_retrieve(vs, user_question, k=10)
    if not docs:
        # return "I don't know.", []
        st.warning("âš ï¸ No documents retrieved for this query.")

    # æ„å»ºä¸Šä¸‹æ–‡å¹¶åŠ å…¥æ¥æºä¿¡æ¯
    blocks, used = [], []
    MAX_CTX = 12000  # é˜²æ­¢è¿‡é•¿è¢«æˆªæ–­
    total = 0
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source", "PDF")
        pg  = d.metadata.get("page", "?")
        block = f"[S{i} | {src} p.{pg}]\n{d.page_content.strip()}"
        if total + len(block) > MAX_CTX:
            break
        blocks.append(block)
        used.append((src, pg))
        total += len(block)

    context_text = "\n\n".join(blocks)

    # æ„å»ºæç¤ºè¯
    prompt = f"""
You are an expert fisheries policy assistant.
Use ONLY the context below. If the context contains relevant facts, answer concisely and cite sources like [S1], [S2] at the end of sentences derived from them.
Only say "I don't know." if the context truly does not contain the answer.

Context:
{context_text}

Question:
{user_question}

Answer (with citations):
""".strip()

    # è°ƒç”¨ Gemini æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
    model = genai.GenerativeModel("gemini-2.5-flash")
    resp = model.generate_content(
        prompt,
        generation_config={
            "temperature": 1.1,
            "max_output_tokens": 4096,  # å¢åŠ è¾“å‡ºé•¿åº¦é™åˆ¶ï¼Œé¿å…ç­”æ¡ˆè¢«æˆªæ–­
        },
    )
    # æå–å›ç­”æ–‡æœ¬
    answer = (getattr(resp, "text", "") or "").strip()
    if not answer:
        answer = "I don't know."
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆï¼Œæ˜¾ç¤ºæœ€è¿‘çš„ç›¸å…³ç‰‡æ®µ
    if answer.strip() == "I don't know." and used:
        hints = [f"â€¢ {s or 'PDF'} p.{p}" for s,p in used[:3]]
        st.info("Closest sections:\n" + "\n".join(hints))

    return answer, used

@st.cache_resource(show_spinner=False)
def build_or_load_vector_store(_documents=None):
    """
    Load existing index if available; otherwise build when documents are provided.
    Returns None if no index exists and no documents are provided (no exception thrown).
    """
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model=EMBED_MODEL)
    except Exception as e:
        st.error(f"âŒ Embeddings initialization failed: {str(e)}")
        return None

    # å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
    if os.path.isdir(INDEX_DIR):
        try:
            return FAISS.load_local(
                INDEX_DIR, embeddings, allow_dangerous_deserialization=False
            )
        except Exception as e:
            # ç‰ˆæœ¬æˆ–æ ¼å¼ä¸å…¼å®¹ï¼Œç»§ç»­å°è¯•é‡å»º
            st.warning(f"âš ï¸ Failed to load existing index, will rebuild: {str(e)}")
            pass

    # æ²¡æœ‰æœ¬åœ°ç´¢å¼•ï¼›è‹¥æä¾›äº†æ–‡æ¡£åˆ™æ„å»ºï¼Œå¦åˆ™è¿”å› None
    if _documents:
        try:
            st.info(f"ğŸ“Š Generating vector index for {len(_documents)} document chunks...")
            vs = FAISS.from_documents(_documents, embedding=embeddings)
            vs.save_local(INDEX_DIR)
            return vs
        except Exception as e:
            st.error(f"âŒ Index building failed: {str(e)}")
            return None

    # æ—¢æ— ç´¢å¼•ï¼Œä¹Ÿæ²¡æ–‡æ¡£ â€”â€” äº¤ç»™ä¸Šå±‚å‹å¥½æç¤º
    return None

# ---------- Streamlit UI ----------
def main():
    st.set_page_config(page_title="AI Fisheries Manager ğŸŸ", page_icon="ğŸŸ")
    st.title("AI Fisheries Manager ğŸŸ")
    st.image("https://pingla.org.au/images/Pingala_Logo_option_7.png", width=300)

    # ä¾§è¾¹æ ï¼šä¸Šä¼ ä¸å»ºç´¢å¼•
    with st.sidebar:
        st.header("Documents")
        pdf_docs = st.file_uploader(
            "Upload PDF files then click 'Submit & Process'",
            type=["pdf"],
            accept_multiple_files=True,
        )

        if st.button("Submit & Process", type="primary", help="Extract, chunk, and index PDFs"):
            if not pdf_docs:
                st.error("Please upload at least one PDF.")
            else:
                with st.spinner("Extracting & indexing..."):
                    try:
                        # æå–æ–‡æ¡£
                        st.info(f"ğŸ“„ Extracting {len(pdf_docs)} PDF file(s)...")
                        raw_docs = get_docs_with_meta(pdf_docs)
                        if not raw_docs:
                            st.error("âŒ Failed to extract text content from PDFs.")
                        else:
                            st.success(f"âœ… Successfully extracted {len(raw_docs)} page(s)")
                            
                            # åˆ†å‰²æ–‡æ¡£
                            splitter = RecursiveCharacterTextSplitter(
                                chunk_size=2500,   # é€‚åº¦æ”¾å¤§
                                chunk_overlap=300, # ä¿ç•™è·¨æ®µè¯­ä¹‰
                                length_function=len,
                            )
                            chunks = splitter.split_documents(raw_docs)
                            st.info(f"ğŸ“ Documents split into {len(chunks)} chunk(s)")

                            # é‡å»ºç´¢å¼•ï¼šå…ˆæ¸…æ‰æ—§ç›®å½•
                            if os.path.isdir(INDEX_DIR):
                                shutil.rmtree(INDEX_DIR, ignore_errors=True)

                            # æ„å»ºå‘é‡ç´¢å¼•
                            vs = build_or_load_vector_store(chunks)
                            if vs is None:
                                st.error("âŒ Index building failed. Please check error messages and retry.")
                            else:
                                st.success("âœ… Index built successfully! You can now start asking questions.")
                    except Exception as e:
                        st.error(f"âŒ Processing error: {str(e)}")
                        import traceback
                        st.error(f"Detailed error:\n```\n{traceback.format_exc()}\n```")

        st.divider()
        st.caption("FAISS index detected âœ”ï¸" if os.path.isdir(INDEX_DIR) else
                   "No index yet. Please upload PDFs and build the index.")

    # ä¸»åŒºï¼šé—®ç­”
    user_q = st.text_input("Ask the fisheries manager a question")
    if user_q:
        if not os.path.isdir(INDEX_DIR):
            st.warning("No index found. Please upload PDFs and click 'Submit & Process' first.")
            return
        with st.spinner("Retrieving & answering..."):
            vs = build_or_load_vector_store()  ##optimize 
            if vs is None:
                st.warning("Index not ready. Please upload PDFs and click 'Submit & Process' first.")
                st.stop()
                
            answer, sources = answer_question(vs, user_q)

        st.markdown("**Reply:**")
        st.write(answer)

        if sources:
            src_text = ", ".join([f"{s or 'PDF'} p.{p}" for s, p in sources])
            st.caption(f"Sources: {src_text}")


if __name__ == "__main__":
    main()